================================================================================
  CHAPTER PLAN — Nature Machine Intelligence Article Format (Simplified)
  Paper: Toxicity Classification for Small Molecules and Peptides
  Author: Jakov Cvetko, Faculty of Engineering (RITEH), University of Rijeka
================================================================================

TARGET JOURNAL: Nature Machine Intelligence (Springer Nature, Nature Portfolio)
REFERENCE STYLE: Nature Portfolio Numbered (sn-nature.bst)
TEMPLATE: sn-jnl.cls (Springer Nature Journal Article Class)
PAGE LIMIT: 8 pages maximum

STRUCTURE MODELLED AFTER:
  Njirjak et al. (2024) Nat. Mach. Intell. 6, 1487–1500
    → Introduction → Results and discussion → Conclusion → Methods
  Leyva et al. (2025) Nat. Mach. Intell. 7, 1685–1697
    → Introduction → Results → Discussion → Methods

  Both papers live in paper/example-works/ — consult before writing.

================================================================================
  SECTION-BY-SECTION PLAN (4 sections only)
================================================================================

1. ABSTRACT (~150-200 words, unstructured, no citations)
   Cover: problem, approach, key result, significance.

--------

2. INTRODUCTION (~0.75 pages, 3 concise paragraphs)
   P1 — Molecular toxicity prediction, computational approaches,
         GNNs as natural molecular representation, DeepGraphCNN.
   P2 — Transfer learning opportunity: shared chemical basis,
         gap in cross-domain TL for toxicity.
   P3 — What this study does: bidirectional TL, 4 strategies,
         multiple model capacities, rigorous CV, SVM baseline.
   Voice: Passive / "this study" (solo author, no "we").
   Status: DONE

--------

3. RESULTS AND DISCUSSION (~3-3.5 pages, combined section)
   Weave findings and interpretation together. Use descriptive
   subheadings (not numbered) to organize content:

   Dataset characteristics
     - Table 1: summary statistics for both datasets
     - Shared 72-element atom vocabulary enabling cross-domain transfer

   Transfer learning performance
     - Table 2: peptide target results (SMT→Peptide)
     - Table 3: SMT target results (Peptide→SMT)
     - Compare methods vs baseline, discuss which strategies work
     - Discuss directional asymmetry (RQ4)
     - Figure 3: radar chart peptide, Figure 4: radar chart SMT

   Effect of model capacity
     - How Standard, Large Layers, Inflated, Extra Inflated compare
     - Whether larger models benefit more from transfer

   Statistical analysis
     - Friedman test results, Nemenyi post-hoc comparisons
     - Which differences are significant (RQ3)

   Comparison with DPC+SVM baseline
     - GNN baseline vs SVM vs best TL method on peptide domain (RQ6)

   Key insights
     - Answer RQ1-RQ6 naturally within the narrative
     - Limitations (brief)

--------

4. CONCLUSION (~0.25-0.5 pages, 1 paragraph)
   - Key takeaways
   - Practical implications for molecular screening
   - Future directions (other architectures, more domains)

--------

5. METHODS (~2-2.5 pages, descriptive subheadings)

   Datasets
     - Sources, sizes, class distributions, SMILES representation

   Molecular graph construction
     - SMILES → RDKit → graph; 72-element one-hot node features

   DeepGraphCNN architecture
     - Layer stack, model capacity configurations
     - Table or inline: 4 model sizes

   Source model pretraining
     - 100% source data, 90/10 split, early stopping patience=7

   Transfer learning strategies
     - M1: Freeze GNN; M2: Freeze Readout; M3: Freeze All + new output;
       M4: Gradual Unfreezing (3 phases)
     - Rationale for each, brief

   DPC+SVM reference model
     - 400-dim DPC, RBF SVM, aligned CV splits

   Cross-validation and evaluation
     - Stratified 10-fold CV, shared splits
     - Metrics: ROC-AUC, G-Mean, Precision, Recall, F1, MCC
     - Statistical testing: Friedman + Nemenyi

--------

6. BACKMATTER
   - Acknowledgements (if applicable)
   - Declarations (funding, competing interests, data/code availability)
   - References (sn-bibliography.bib, Nature numbered style)

================================================================================
  FIGURES — FINAL (4 figures, all PNGs from paper/images/)
================================================================================

  Fig. 1 — Project pipeline overview         (project-diagram.png)
  Fig. 2 — Model architectures side-by-side  (side-by-side-models-boxes.png)
  Fig. 3 — Radar chart, peptide target       (radar_plots_peptide_median.png)
  Fig. 4 — Radar chart, SMT target           (radar_plots_smt_median.png)

================================================================================
  TABLES — FINAL (exactly 3 tables)
================================================================================

  Table 1 — Dataset characteristics          (dataset_analysis_results.md)
  Table 2 — Peptide target results           (eval-book-peptide-results.xlsx)
  Table 3 — SMT target results               (eval-book-smt-results.xlsx)

================================================================================
  KEY WRITING RULES
================================================================================

  - Solo author: NO "we". Use passive voice or "this study"/"the present work"
  - Concise: every sentence must carry information
  - Quantitative: always back claims with metric values (mean +/- std)
  - Consistent terminology: see PAPER_GUIDE.md Section 6.3
  - Shared vocabulary = 72 elements (any doc saying 27 is a typo)
  - "Pretrained source model" not "overtrained model"
  - Always specify transfer direction when reporting results
  - Only 4 \section{} commands in the body — no more
  - Descriptive subheadings within sections, never numbered subsections
  - Consult paper/example-works/ before writing ANY section

================================================================================
